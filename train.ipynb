{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dis\n",
    "from taxi_env import TaxiEnv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchPolicy:\n",
    "    def __init__(self, state_size, action_size, lr = 0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(state_size, state_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(state_size // 2, action_size),\n",
    "        ).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr = lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype = torch.float, device = device)\n",
    "        probs = nn.functional.softmax(self.policy(state), dim = 0)\n",
    "        m = dis.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        state = torch.tensor(state, dtype = torch.float, device = device)\n",
    "        action = torch.tensor(action, dtype = torch.long)\n",
    "        \n",
    "        loss = reward * self.criterion(self.policy(state).cpu(), action)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "def train(fuel_limit = 5000, episodes = 5000, lr = 0.001, gamma = 0.99):\n",
    "    env = TaxiEnv(fuel_limit)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    state_size = len(get_state(obs))\n",
    "    action_size = 6\n",
    "\n",
    "    policy_model = PyTorchPolicy(state_size, action_size, lr = lr)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    pickup_per_episode = []\n",
    "    step_per_episode = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        first_pickup = True\n",
    "        first_visit = [True] * 4\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = policy_model.get_action(state)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = get_state(obs)\n",
    "            state = next_state\n",
    "\n",
    "            if env.passenger_picked_up:\n",
    "                reward += 0.05\n",
    "                if first_pickup:\n",
    "                    reward += 200\n",
    "                    first_pickup = False\n",
    "            for i in range(0, 4, 2):\n",
    "                if first_visit[i]:\n",
    "                    if state[i] == 0 and state[i + 1] == 0:\n",
    "                        reward += 50\n",
    "                        first_visit[i] = False\n",
    "            if terminated:\n",
    "                reward += 1000\n",
    "\n",
    "            total_reward += reward\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # print(state)\n",
    "            # env.render(action, reward)\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        pickup_per_episode.append(not first_pickup)\n",
    "        step_per_episode.append(len(rewards))\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        loss = torch.stack(log_probs) * -returns\n",
    "        loss = loss.sum()\n",
    "\n",
    "        policy_model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_model.optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Average Reward: {np.mean(rewards_per_episode[-10:]):07.3f}, Average Step: {np.mean(step_per_episode[-10:]):05f}, Pickup Count: {np.count_nonzero(pickup_per_episode[-10:]):02d}, Success count: {np.count_nonzero(np.array(step_per_episode[-10:]) < fuel_limit):02d}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/5000, Average Reward: -4775.990, Average Step: 1000.000000, Pickup Count: 01, Success count: 00\n",
      "Episode 20/5000, Average Reward: -4427.140, Average Step: 1000.000000, Pickup Count: 04, Success count: 00\n",
      "Episode 30/5000, Average Reward: -4402.735, Average Step: 1000.000000, Pickup Count: 04, Success count: 00\n",
      "Episode 40/5000, Average Reward: -3828.535, Average Step: 921.300000, Pickup Count: 04, Success count: 01\n",
      "Episode 50/5000, Average Reward: -4513.905, Average Step: 908.400000, Pickup Count: 06, Success count: 01\n",
      "Episode 60/5000, Average Reward: -4413.900, Average Step: 1000.000000, Pickup Count: 04, Success count: 00\n",
      "Episode 70/5000, Average Reward: -3969.525, Average Step: 976.400000, Pickup Count: 03, Success count: 01\n",
      "Episode 80/5000, Average Reward: -3888.135, Average Step: 995.300000, Pickup Count: 05, Success count: 01\n",
      "Episode 90/5000, Average Reward: -4003.470, Average Step: 1000.000000, Pickup Count: 02, Success count: 00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m policy_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuel_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[89], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(fuel_limit, episodes, lr, gamma)\u001b[0m\n\u001b[0;32m     57\u001b[0m first_visit \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 60\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     62\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[1;32mIn[89], line 18\u001b[0m, in \u001b[0;36mPyTorchPolicy.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     16\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[0;32m     17\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(state), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mdis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\SB3\\Lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\SB3\\Lib\\site-packages\\torch\\distributions\\distribution.py:70\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m     69\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m---> 70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_model = train(fuel_limit = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_model.policy.state_dict(), \"policy\")\n",
    "torch.save(policy_model.optimizer.state_dict(), \"optimizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
